# Building a GPT from Scratch
This repository is built to deeply understand how Large Language Models work from scratch.


Instead of focusing on scale or performance, it breaks down core components such as tokenization, self-attention, multi-head attention, transformer blocks, and autoregressive text generation.


The goal is clarity: seeing how each part fits together to form a GPT-style language model.
