# Building a GPT from Scratch
This repository is built to deeply understand how Large Language Models work from scratch.


Instead of focusing on scale or performance, it breaks down core components such as tokenization, self-attention, multi-head attention, transformer blocks, and autoregressive text generation.


The goal is clarity: seeing how each part fits together to form a GPT-style language model.
___

## ðŸ“Œ Project Workflow

This project is organized to help you understand each building block of a GPT-style language model independently, while still allowing everything to come together in a complete training pipeline.


The workflow follows the same logical order as a real LLM: