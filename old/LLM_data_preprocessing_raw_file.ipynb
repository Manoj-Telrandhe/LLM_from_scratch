{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuewpUMD7IH0"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/verdict_story.txt\", \"r\") as data_file:\n",
        "  raw_text = data_file.read()\n",
        "\n",
        "print(\"Total number of character: \", len(raw_text))\n",
        "print(raw_text[0:101])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regular expression (re) Python library to split the text to obtain the list of tokens"
      ],
      "metadata": {
        "id": "QMeJ4MReBu19"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "siYTDVdkB-do"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = 'Hello, world. This, is a test.'\n",
        "\n",
        "result = re.split(r'(\\s)', text)  ## here splitting text on whitespace characters:\n",
        "\n",
        "print(result)\n",
        "# print(len(result))"
      ],
      "metadata": {
        "id": "TQPPhli0-33C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4ea4708-92d7-4731-b1ff-2dc252c91ee8"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "token : 'Hello,'\n",
        "we want token: 'Hello', ','\n",
        "We want that commas,  fullstop and punctuations to be separate token split from the text so that LLM will get the idea that what are commas and puctuations.\n",
        "\n",
        "and other thing we want that we don't want that the white spaces to be separate token"
      ],
      "metadata": {
        "id": "MdYzo5ChTJjD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Let's modify the regular expression splits on whitespaces(\\s) and commas, and periods([,.]):"
      ],
      "metadata": {
        "id": "HCOuTwNaCWK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Hello, world. This, is a test.'\n",
        "\n",
        "result = re.split(r'([,.]|\\s)', text)  # split text on whitespaces ',' '.'\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "0rq1jafT-uJ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76026d00-675b-4aa7-935b-99375fbf68b5"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the words and punctuation characters are now separate list entries jus as we wanted."
      ],
      "metadata": {
        "id": "NHRkp02EETi3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####A small remaining issue is that the list still includes whitespaces characters. Optionally, we can rmeove these redudant characters safely as follows:"
      ],
      "metadata": {
        "id": "GYXQDKXHEyPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = [item for item in result if item.strip()]  ## for whitespace item.strip is false\n",
        "result                                              # so we are not returning them or words or '.' or ','"
      ],
      "metadata": {
        "id": "os1OPuRh7K3A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "286d5059-15d0-40ba-820f-62bfec6ed59e"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "REMOVING WHITESPACES OR NOT\n",
        "\n",
        "When developing a simple tokenizer, whether we should encode whitespaces as separate characters or just remove them depends on our application and its requirements. Removing whitespaces reduces the memory and computing requirements. However, keeping whitespaces can be useful if we train models that are sensitive to the exact structure of the text (for example, Python code, which is sensitive to indentation and spacing). Here, we remove whitespaces for simplicity and brevity of the tokenized outputs. Later, we will switch to a tokenization scheme that includes whitespaces."
      ],
      "metadata": {
        "id": "XzxMu7flWxj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# removing white spaces\n",
        "[item for item in result if item.strip()]\n",
        "# item.strip() return a true value if white space is not there (item.strip() = True)\n",
        "# so where ever white spacces are there it outputs a False value  (item.strip() = False)"
      ],
      "metadata": {
        "id": "z-dEF-ArFA9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenization scheme we devised above works well on the simple sample text. Let's modify it a bit further so that it can also handle other types of punctuation, such as question marks, quotation marks, and the double-dashes we have seen earlier in the first 100 characters of Edith Wharton's short story, along with additional special characters:"
      ],
      "metadata": {
        "id": "wdOD2LufHMaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, World. Is this-- a test?\"\n",
        "\n",
        "result = re.split(r'([,.:;?_!()\\']|--|\\s)', text) # text will get split on ,.:;?_!()\\']|--|\\s\n",
        "print(result)"
      ],
      "metadata": {
        "id": "f1fr5_B-FA58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to remove the whitespaces from each item and then filter out any empty strings.\n",
        "result = [item for item in result if item.strip()]\n",
        "result"
      ],
      "metadata": {
        "id": "sil23qUTFAza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# So in two lines we have build tokenizer\n",
        "\n",
        "text = \"Hello, World. Is this-- a test?\"\n",
        "\n",
        "result = re.split(r'([,.:;?_!()\\']|--|\\s)', text)\n",
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "id": "wzK5g6n_7Kzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PaEZj6MsaWhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Now we got a basic tokenizer working, let's apply it to Edith Wharton's text"
      ],
      "metadata": {
        "id": "QHaL3nu4Imuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed = re.split(r'([,.:;?_!()\\']|--|\\s)', raw_text) #split will happen on ,.:;?_!()\\']|--|\\s\n",
        "preprocessed = [item for item in preprocessed if item.strip()] #removed whitespaces\n",
        "\n",
        "print(preprocessed[:30])  # print first 30 tokens to illustrate"
      ],
      "metadata": {
        "id": "pOkDTARQ7Kws",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50127f1c-dc79-4987-d405-3567cd9d43f2"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(len(preprocessed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85_kSPg5WI5z",
        "outputId": "cd42a742-a8de-4f3e-b38a-2b294259dbde"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4606\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 2: Creating Token"
      ],
      "metadata": {
        "id": "UmNftlGDJlUC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous section, we tokenized Edith Wharton's short story and assigned it to a Python variable called preprocessed. Let's now create a list of all unique tokens and sort them alphabetically to determine the vocabulary size:"
      ],
      "metadata": {
        "id": "g5X7jgyTXA5S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a list of tuple of all unique token and sort them alphabetically to determine the vocabulary size:"
      ],
      "metadata": {
        "id": "5IfcrzjG2fpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = sorted(set(preprocessed))\n",
        "\n",
        "vocab_size = len(all_words)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "id": "vzBkBaPxJXMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After determining that the vocabulary size is 1,130 via the above code, we create the vocabulary and print its first 51 entries for illustration purposes:"
      ],
      "metadata": {
        "id": "WJnoo06DXFqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mapping of words/tokens to token IDs\n",
        "vocab = {token: integer for integer, token in enumerate(all_words)}\n",
        "#dictionary comprehension"
      ],
      "metadata": {
        "id": "RVyUCtEEM-pP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1. enumerate(all_words)\n",
        "# This converts your list of unique words into pairs:\n",
        "# [(0, 'apple'), (1, 'boy'), (2, 'cat'), ...]\n",
        "\n",
        "#2. Dictionary Comprehension {token: integer for ...}\n",
        "# This takes each (integer, token) pair and creates a dictionary entry:\n",
        "    # 'word' : id\n",
        "    # {\n",
        "    #     'apple': 0,\n",
        "    #     'boy': 1,\n",
        "    #     'cat': 2,\n",
        "    #      ...\n",
        "    # }\n",
        "\n",
        "\n",
        "# 3. simple code for dictionary comrehension\n",
        "vocab = {}\n",
        "for integer, token in enumerate(all_words):\n",
        "  vocab[token] = integer\n"
      ],
      "metadata": {
        "id": "pDcvlZnE5kG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(vocab.items()):\n",
        "  print(item)\n",
        "  if i>=50:\n",
        "    break"
      ],
      "metadata": {
        "id": "IhgJXV05M9en"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, based on the out above, the dict contains individual tokens associated with unique integer labels."
      ],
      "metadata": {
        "id": "b5cblWtM70yv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Later in this book, when we want to convert the outputs of an LLM from numbers back into text, we also need a way to turn token IDs into text.\n",
        "\n",
        "For this, we can create an inverse version of the vocabulary that maps token IDs back to corresponding text tokens.\n",
        "\n"
      ],
      "metadata": {
        "id": "VB4DmpmCX7JU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's implement a complete tokenizer class in Python.\n",
        "\n",
        "The class will have an encode method that splits text into tokens and carries out the string-to-integer mapping to produce token IDs via the vocabulary.\n",
        "\n",
        "In addition, we implement a decode method that carries out the reverse integer-to-string mapping to convert the token IDs back into text."
      ],
      "metadata": {
        "id": "KqUNoBe5X_MX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Store the vocabulary as a class attribute for access in the encode and decode methods\n",
        "\n",
        "Step 2: Create an inverse vocabulary that maps token IDs back to the original text tokens\n",
        "\n",
        "Step 3: Process input text into token IDs\n",
        "\n",
        "Step 4: Convert token IDs back into text\n",
        "\n",
        "Step 5: Replace spaces before the specified punctuation"
      ],
      "metadata": {
        "id": "zsa0zZrrYHWD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "create a class of Tokenizer"
      ],
      "metadata": {
        "id": "dKapd6Q3LRnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV1:\n",
        "  def __init__(self, vocab):\n",
        "    self.str_to_int = vocab    ##vocab is mapping of str to int so str_to_int is vocab directly\n",
        "    self.int_to_str = {i: s for s,i in vocab.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.:;?_!()\\']|--|\\s)', text) #split the text\n",
        "\n",
        "    preprocessed = [\n",
        "        item.strip() for item in preprocessed if item.strip()  ## removing white spaces\n",
        "    ]\n",
        "\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])  #get the tokens from ids and join those tokens\n",
        "    # Replace spaces which are present before specified punctuations\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "MDttM9KkLP6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "so in above class it already takes vocab as input.\n",
        "\n",
        "so we already have the mapping of tokens to IDs, because that is present in the vocabulary.\n",
        "\n",
        "we just need to convert the reverse, from the token IDs from the token and in decode method\n",
        "\n",
        "and in text to token we did same re.split and item.strip\n"
      ],
      "metadata": {
        "id": "lL0X591ZPYm9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's instantiate a new tokenizer object from the SimpleTokenizerV1 class and tokenize a passage from Edith Wharton's short story to try it out in practice:"
      ],
      "metadata": {
        "id": "pRh2ZAEZYQSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "# testing the encode method by passing the sample text\n",
        "# text is from \"training set\"\n",
        "text = \"\"\"\"It's the last he painted, you know,\"\n",
        "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "id": "weoRbW_dLP2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code above prints the following token IDs: Next, let's see if we can turn these token IDs back into text using the decode method:"
      ],
      "metadata": {
        "id": "sis5ha28YW8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test the decode method\n",
        "text = tokenizer.decode(ids)\n",
        "print(text)"
      ],
      "metadata": {
        "id": "1FBK4C_D31gi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "1a1a2b34-78c6-454e-eef8-4fd093d60d0f"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'ids' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-488628390.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# test the decode method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ids' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above encode and decode doing perfect job of tokenizing and de-tokenizing when we gave the text snipet from the training set."
      ],
      "metadata": {
        "id": "uR59msJXVooN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "so we have implemented the tokenizer capable of tokenizing and de-tokenizing the text based on a snipet from the training set.\n",
        "\n",
        "Let's now apply it to a new text sample that is not contained in the training set:"
      ],
      "metadata": {
        "id": "Adv87aJcV2_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, do you like tea?\"\n",
        "print(tokenizer.encode(text))"
      ],
      "metadata": {
        "id": "VTm39kwdVngR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The problem is that the word \"Hello\" is not used on the verdict short story.\n",
        "\n",
        "Hence, it is not contained in the vocabulary.\n",
        "\n",
        "This highlights the need tp consider large and diverse training set to extend the vocabulary when working on LLMs."
      ],
      "metadata": {
        "id": "PYI23McOd3EK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ADDING SPECIAL CONTEXT TOKENS"
      ],
      "metadata": {
        "id": "yh0BRn_ceULY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous section, we implemented a simple tokenizer and applied it to a passage from the training set.\n",
        "\n",
        "In this section, we will modify this tokenizer to handle unknown words.\n",
        "\n",
        "In particular, we will modify the vocabulary and tokenizer we implemented in the previous section, SimpleTokenizerV2, to support two new tokens, <|unk|> and <|endoftext|>"
      ],
      "metadata": {
        "id": "gmAozJWXYrAy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can modify the tokenizer to use an <|unk|> token if it encounters a word that is not part of the vocabulary.\n",
        "\n",
        "Furthermore, we add a token between unrelated texts.\n",
        "\n",
        "For example, when training GPT-like LLMs on multiple independent documents or books, it is common to insert a token before each document or book that follows a previous text source"
      ],
      "metadata": {
        "id": "oHWkExM9Yuvu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now modify the vocabulary to include these two special tokens, and <|endoftext|>, by adding these to the list of all unique words that we created in the previous section:"
      ],
      "metadata": {
        "id": "Uihx3ERbYxwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessed"
      ],
      "metadata": {
        "id": "tuOdUj5KsSQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|unk|>\", \"<|endoftext|>\"])\n",
        "\n",
        "vocab = {token:integer for integer, token in enumerate(all_tokens)} #map token to token IDs"
      ],
      "metadata": {
        "id": "W8y6ZquTUYsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab.items())"
      ],
      "metadata": {
        "id": "Y1MFqqNssWa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "earlier the vocab size was 1158 now we added two tokens so the new vocabulary size is 1160"
      ],
      "metadata": {
        "id": "0oTlhEl-s6EJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an additional quick check, let's print the last 5 entries of the updated vocabulary:"
      ],
      "metadata": {
        "id": "YvTqa3W8tLgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token, integer in list(vocab.items())[-5:]:\n",
        "  print(token, integer)"
      ],
      "metadata": {
        "id": "L1GFYI_tszX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now let's create simple tokenizer class which can handle unknown words"
      ],
      "metadata": {
        "id": "oJqsSodBt8zl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Replace unknown words by <|unk|> tokens\n",
        "\n",
        "Step 2: Replace spaces before the specified punctuations"
      ],
      "metadata": {
        "id": "7UFbK3BeY8Pm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]  # remove whitespaces\n",
        "        preprocessed = [\n",
        "            item if item in self.str_to_int\n",
        "            else \"<|unk|>\" for item in preprocessed\n",
        "        ]\n",
        "\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])  #join tokens using the .join using spaces \" \".join\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "EZoZu0W1zLXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)"
      ],
      "metadata": {
        "id": "_McReh2-uanw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "id": "_tEExCaDwD00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(text)"
      ],
      "metadata": {
        "id": "q-8Xc797wJab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# de-tokenize\n",
        "tokenizer.decode(tokenizer.encode(text))"
      ],
      "metadata": {
        "id": "tMFJeg2Py1mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on comparing the de-tokenized text above with the original input text, we know that the training dataset, Edith Wharton's short story The Verdict, did not contain the words \"Hello\" and \"palace.\""
      ],
      "metadata": {
        "id": "37FozmRoznYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, we have discussed tokenization as an essential step in processing text as input to LLMs. Depending on the LLM, some researchers also consider additional special tokens such as the following:\n",
        "\n",
        "[BOS] (beginning of sequence): This token marks the start of a text. It signifies to the LLM where a piece of content begins.\n",
        "\n",
        "[EOS] (end of sequence): This token is positioned at the end of a text, and is especially useful when concatenating multiple unrelated texts, similar to <|endoftext|>. For instance, when combining two different Wikipedia articles or books, the [EOS] token indicates where one article ends and the next one begins.\n",
        "\n",
        "[PAD] (padding): When training LLMs with batch sizes larger than one, the batch might contain texts of varying lengths. To ensure all texts have the same length, the shorter texts are extended or \"padded\" using the [PAD] token, up to the length of the longest text in the batch."
      ],
      "metadata": {
        "id": "g5ArJ-oOZfsJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the tokenizer used for GPT models does not need any of these tokens mentioned above but only uses an <|endoftext|> token for simplicity"
      ],
      "metadata": {
        "id": "i99OKEBWZk4l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the tokenizer used for GPT models also doesn't use an <|unk|> token for outof-vocabulary words. Instead, GPT models use a byte pair encoding tokenizer, which breaks down words into subword units"
      ],
      "metadata": {
        "id": "RRE527J2ZoCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##BYTE PAIR ENCODING"
      ],
      "metadata": {
        "id": "DCJ76r4Qzp_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BPE Tokenizer**"
      ],
      "metadata": {
        "id": "PMeuB5SgzxIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the verdict story text data\n",
        "with open(\"/content/Data.txt\", \"r\") as data_file:\n",
        "  raw_text = data_file.read()\n",
        "\n",
        "print(\"Total number of character: \", len(raw_text))\n",
        "# print(raw_text[0:101])"
      ],
      "metadata": {
        "id": "XRchzmsXy6mJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "lnc9SqMoBs_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "import importlib\n",
        "\n",
        "# check the versio of tiktokens\n",
        "print(\"tiktokens version:\", importlib.metadata.version(\"tiktoken\"))"
      ],
      "metadata": {
        "id": "L5LOQFFLEI6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "bCRGyDd7FTLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # to get the tokeniser correspoding to a specific model in the openAI API:\n",
        "# tokenizer = tiktoken.encoding_for_model(\"gpt-4o\")"
      ],
      "metadata": {
        "id": "xqDU7K7evzpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello do you like tea ? <|endoftext|> In the sunlit terrace of someunknownPlace\"\n",
        "\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "print(integers)"
      ],
      "metadata": {
        "id": "bswTHlqUrHpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the code above prints the token IDs"
      ],
      "metadata": {
        "id": "GgDIIH5DtLjS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can convert the token IDs back into text using the decode method, similar to our SimpleTokenizer"
      ],
      "metadata": {
        "id": "C2UF8BW4tPJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string = tokenizer.decode([6])\n",
        "print(string)\n",
        "\n",
        "string = tokenizer.decode([8])\n",
        "print(string)\n",
        "\n",
        "string = tokenizer.decode([50])\n",
        "print(string)\n",
        "\n",
        "# decode the bove text\n",
        "string = tokenizer.decode(integers)\n",
        "print(string)"
      ],
      "metadata": {
        "id": "5qVGFd5ErJP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "let's take another example to illustrate how BPE deals with unknown words"
      ],
      "metadata": {
        "id": "l9w6IsI92E2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "integers = tokenizer.encode(\"htyhty ier\")\n",
        "print(integers)\n",
        "\n",
        "string = tokenizer.decode(integers)\n",
        "print(string)"
      ],
      "metadata": {
        "id": "VKKqVZbbrJMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "integers = tokenizer.encode(\"babaYaga\")\n",
        "print(integers)\n",
        "\n",
        "string = tokenizer.decode(integers)\n",
        "string"
      ],
      "metadata": {
        "id": "pT9_571jrJJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OV-bUrJI5LBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lecture"
      ],
      "metadata": {
        "id": "UWdjiuIrnv00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create Input-Target pairs"
      ],
      "metadata": {
        "id": "kD-aLVGysukF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "input-target pars using sliding window approach"
      ],
      "metadata": {
        "id": "UbD65Hr4vbvv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "first tokenize the verdict story (text data) earlier we took BPE tokenizer that we introduced"
      ],
      "metadata": {
        "id": "0_JZoZTYv013"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "YAzg3Ku0n1J5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/verdict_story.txt\", 'r') as file:\n",
        "  raw_text = file.read()\n",
        "\n",
        "# print(raw_text)\n",
        "# tokenizer\n",
        "\n",
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(len(enc_text))\n",
        "print(max(enc_text))\n",
        "print(enc_text)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "EnOTp_0SCPJv",
        "outputId": "78849546-2970-4351-efca-293129dd6e65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'<|unk|>'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3614160920.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0menc_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-84565434.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     12\u001b[0m         ]\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: '<|unk|>'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the above code will return 5147, the total number of tokens in the training set after applying the BPE encoder"
      ],
      "metadata": {
        "id": "PaqhS89FxrWF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we remove the first 50 tokens from the dataset for the demostration purpose as it result in slightly more interesting text passage .\n",
        "\n",
        "we can keep entire tokens also"
      ],
      "metadata": {
        "id": "P3NzBARNyDRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enc_sample = enc_text[50:]\n",
        "len(enc_sample)"
      ],
      "metadata": {
        "id": "0zk0AURMsuHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "one of the easiest and most intuitive ways to create the input-target pairs for the next word prediction tasks is to create two variables, x and y, where as x contains input tokens and y contains the targets, which are the input shifted by 1:"
      ],
      "metadata": {
        "id": "MmrfsGvbzsHN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the context size determines how many words/tokens are included in the input"
      ],
      "metadata": {
        "id": "M2OsUjR00hH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 4 #length of the input\n",
        "# The context size of 4 means that the model is trained to look at a sequence of 4 words (or tokens)\n",
        "# to predict the next word in the sequence.\n",
        "# The input x is the first 4 tokens [1, 2, 3, 4], and the target y is the next 4 tokens [2, 3, 4, 5]\n",
        "\n",
        "x = enc_sample[ : context_size]\n",
        "y = enc_sample[1: context_size+1]\n",
        "\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y:      {y}\")"
      ],
      "metadata": {
        "id": "kZKBwccAygBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "processing the inputs along with the targets, which are the inputs shifted by one position, we can then create then create the next-word prediction tasks as follows:"
      ],
      "metadata": {
        "id": "4I53CkGJ3szt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "  context = enc_sample[ : i]\n",
        "  desired = enc_sample[i]\n",
        "\n",
        "  print(context,  \"----->\", desired)\n"
      ],
      "metadata": {
        "id": "UD0j31Va207b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Everything left of the arrow(--->) refers to the input LLM would recieve, and the token ID on the right side of the arrow represents the target token ID that the LLM is supposed to predict"
      ],
      "metadata": {
        "id": "JLxvcJ9a4sp1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "for illustration purpose let's repreat the previous code but convert the tokens IDs into text:"
      ],
      "metadata": {
        "id": "sp2RUYZxNBEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "  context = enc_sample[ : i]\n",
        "  desired = enc_sample[i]\n",
        "\n",
        "  context = tokenizer.decode(context)  # decode\n",
        "  desired = tokenizer.decode([desired]) #decode\n",
        "\n",
        "  print(context,  \"----->\", desired)"
      ],
      "metadata": {
        "id": "CmsnqiQr4otI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we hav created input-target pairs that LLM uses for training in upcoming chapters."
      ],
      "metadata": {
        "id": "Tft46iWCO-iL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is only one more task before we can turn the tokens into embeddings: implementing an efficient data loader that iterated over the input dataset and returns the inputs and targets as PyTorch tensors, which can be thought of as multi-dimensionals arrays."
      ],
      "metadata": {
        "id": "cvYNNvD8OE9d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In particular, we are interested in returning two tensors: an input tensor containing the text that the LLM sees and a target tensor that includes the targets for the LLM to predict."
      ],
      "metadata": {
        "id": "-BL1nP6tOvzq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1jC7BZ8dmt7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rBAWycYvmt31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lecture dataset and dataloader"
      ],
      "metadata": {
        "id": "wsxLJLjWmtge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###IMPLEMENTING A DATA LOADER"
      ],
      "metadata": {
        "id": "AxDVDBSEPqeW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "for the efficient data loader implementation we will use PyTorch's built in Dataset and DataLoader classes."
      ],
      "metadata": {
        "id": "5KSrPAY9P89X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step1: Tokenize the text\n",
        "\n",
        "Step2: Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "\n",
        "Step3: Return the total number of rows in the dataset\n",
        "\n",
        "Step4: Return a single row of input and output from the dataset."
      ],
      "metadata": {
        "id": "k-IJ2TagP855"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def __len__(self):   # return the total number of rows in the dataset\n",
        "  return len(self.input_ids)"
      ],
      "metadata": {
        "id": "un4Cbaab204N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we have to define a method called get item , what is does is\n",
        "# we provide the index(idx) and it will return that particular row of the input and that particular row of the output tensor\n",
        "\n",
        "def __getitem__(self, idx):\n",
        "  return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "\n",
        "# why it is needed because when we create a DataLoader it will look this method of dataset class,\n",
        "# and then only it will able to create input-output pair one after another,\n",
        "# because this fxn is returning the inp-out pair according to give idx number\n",
        "\n",
        "# Dataloader needs the dataset to be in map style or iterable style\n",
        "# we are using map style dataset"
      ],
      "metadata": {
        "id": "3wTYNlOn201L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# txt = input text\n",
        "# tokenizer = tokenizer to tokenize the text into token ids\n",
        "# max_length = context size\n",
        "# stride = window will slide after that many token (= 1 or context size)\n"
      ],
      "metadata": {
        "id": "q1K0Fn3sv8tV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tiktoken"
      ],
      "metadata": {
        "id": "uh1MHFid34Xw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDataset(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "JSzA_2C5vRu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above GPTDataset class is based on the PyTorch Dataset class.\n",
        "\n",
        "It defines how the individual rows are fetched from the dataset.\n",
        "\n",
        "Each row consists of a number of token IDs (based on the max_length/context size) assigned to input chunk tensor.\n",
        "\n",
        "The target_chunk tensor contains the correspoding targets.\n",
        "\n"
      ],
      "metadata": {
        "id": "CJbk-8-8ugR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ok\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "076-PFJS1SRH",
        "outputId": "55be45de-d4df-4ef2-e9e0-d54b8b0939b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ok\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code will use the Dataset to load the inputs in batches via PyTorch DataLoader:"
      ],
      "metadata": {
        "id": "SdSJoBxF1F9x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step1: Initialize the tokenizer\n",
        "\n",
        "Step2: Create the datset instance\n",
        "\n",
        "Step3: drop_last=True, drops the last batch if if is shorter than the specified batch_size to prevent loss spikes during training\n",
        "\n",
        "Step4: The number of CPU process to use for preprocessing."
      ],
      "metadata": {
        "id": "Ycy5FqqI1R1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# it will help to create input-output pairs\n",
        "# and help to create batches\n",
        "# batch_size = that CPU's processs runs parallely\n",
        "\n",
        "def create_dataloader(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
        "\n",
        "  # Initialize the tokenizer\n",
        "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "  # create dataset instance\n",
        "  dataset = GPTDataset(txt, tokenizer, max_length, stride)\n",
        "\n",
        "  # create dataloader\n",
        "  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
        "\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "nhz4I4Z9mvZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now test the dataloader with the batch_size of 1 for an LLM with a context size of 4,\n",
        "\n",
        "This will develop an intuition of how the GPTDataset class and create_dataloader function will work together:\n"
      ],
      "metadata": {
        "id": "-nWNEdAq5P_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/verdict_story.txt\", \"r\") as file:\n",
        "  raw_text = file.read()"
      ],
      "metadata": {
        "id": "iSCX2DzjmvW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert dataloader into a python iterator to fetch the next entry via Python's built in next() function"
      ],
      "metadata": {
        "id": "hlWr9Jim6V98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader(\n",
        "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZO1qcWskmvTt",
        "outputId": "4c6f8589-a58b-408b-b0db-74b23088d272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first_batch variable contains two tensors: first tensor stores input tokens and second tensor stores the target token IDs\n",
        "\n",
        "Since max_length is set to 4, each two tensors contains 4 tokens IDs\n",
        "\n",
        "max_length/input size of 4 is relatively small and only chose for illustration purpose. It is common to train LLMs with input sizes of at least 256"
      ],
      "metadata": {
        "id": "ZtDTB6fz9R2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To illustrate the meaning of stride=1 let's fetach another batch from this dataset:"
      ],
      "metadata": {
        "id": "8qOBQtoE5UF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "second_batch = next(data_iter)\n",
        "print(second_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rnx3QzgK7RH8",
        "outputId": "f614f903-de35-487a-b0f8-1186053255bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The stride setting dictates the number of positions the inputs shift across the batches, emulating a sliding window approach."
      ],
      "metadata": {
        "id": "WriKAk1P5koV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch size of 1, such as we sampled from the data loader so far, are useful for illustration purpose.\n",
        "\n",
        "We know from the experience with deep learning, that small batch sizes requires less memory during training but lead to more noisy model updates.\n",
        "\n",
        "Just like in regular deep learning, the batch size is a trade-off and hyperparameter to experiment with when training LLMs.\n",
        "\n",
        "(batch size is the number of data model has to process beefore updating its params so\n",
        "  If the batch size is too small the parameters updates is very quick but the updates will be noisy,\n",
        "  If the batch size is very large the model will go through entire dataset before updating its params, it is not effective that's why batch concept came into picture.)"
      ],
      "metadata": {
        "id": "un4dBkUl6iGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we move on to the two final sections of this chapter that are focused on creating token IDs, let's have a brief look at how we can use the data loader to sample a batch size greater than 1:"
      ],
      "metadata": {
        "id": "BjOPFFoM7ovp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqS9NI4H8d1w",
        "outputId": "55137d54-11a3-488e-89e0-0aa3d23b6b5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Targets:\n",
            " tensor([[  367,  2885,  1464,  1807],\n",
            "        [ 3619,   402,   271, 10899],\n",
            "        [ 2138,   257,  7026, 15632],\n",
            "        [  438,  2016,   257,   922],\n",
            "        [ 5891,  1576,   438,   568],\n",
            "        [  340,   373,   645,  1049],\n",
            "        [ 5975,   284,   502,   284],\n",
            "        [ 3285,   326,    11,   287]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note In the above code we increase the stride to 4. TO utilize the dataset fully (we don't skip a single word) but also avoid any overlap between the batches, since more overlap could lead to increased overfitting."
      ],
      "metadata": {
        "id": "Y4CM0uKZ-Jfz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lecture"
      ],
      "metadata": {
        "id": "NdJshO5emlHT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create TOKEN EMBEDDINGS"
      ],
      "metadata": {
        "id": "POzTyI0fE4zX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Small hands on demo playing with embeddings"
      ],
      "metadata": {
        "id": "WObku0L7jiKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYVbOVv5f6A5",
        "outputId": "7fd87438-4f2c-43d7-fd87-15e5266fcd96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "model = api.load(\"word2vec-google-news-300\")  #download the model and return as object ready for use"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "JL0aKy0dE4ay",
        "outputId": "e0fb777f-7b68-4fe1-e609-96b1b7f82f66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==------------------------------------------------] 5.7% 94.2/1662.8MB downloaded"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1623846558.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word2vec-google-news-300\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#download the model and return as object ready for use\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/downloader.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, return_path)\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         \u001b[0m_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/downloader.py\u001b[0m in \u001b[0;36m_download\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{fname}.gz\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0mdst_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_progress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_calculate_md5_checksum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_get_checksum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0mblocknum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m                     \u001b[0mreporthook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocknum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mread\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/downloader.py\u001b[0m in \u001b[0;36m_progress\u001b[0;34m(chunks_downloaded, chunk_size, total_size, part, total_parts)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 round(float(total_size) / (1024 * 1024), 1))\n\u001b[1;32m    128\u001b[0m         )\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         sys.stdout.write(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0;31m# and give a timeout to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m                 \u001b[0;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# word2vec-google-news-300 pretrained model by google each word is mapped to 300 dim vector"
      ],
      "metadata": {
        "id": "e5KlIvJvqT7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of a word as a vector"
      ],
      "metadata": {
        "id": "8M4xRJcVj9sn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors = model\n",
        "# word_vectors is a dictionary because each word is getting mapped to 300 dim vector\n",
        "\n",
        "# Let's us look how the vector embedding of a word looks like\n",
        "print(word_vectors['computers']) #.Example: accesing the vector for the word 'computer'"
      ],
      "metadata": {
        "id": "Sq0xWvAL9boJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_vectors['cat'].shape)  # see each word is maaped to 300dim vector"
      ],
      "metadata": {
        "id": "aFC_nmh_kd4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####King + Women - Man = ?"
      ],
      "metadata": {
        "id": "xv5OnHa3lAlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Example of using most_similar\n",
        "print(word_vectors.most_similar(positive=['king', 'women'], negative=['men'], topn=10))  # top10 most similar words\n"
      ],
      "metadata": {
        "id": "WzOzTBnDkzy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# top 10 with percentage\n",
        "print(word_vectors.most_similar(positive=['king', 'women'], negative=['women'], topn=10))"
      ],
      "metadata": {
        "id": "XMckS8j6mDZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Let check similarity b/w a few pairs of words"
      ],
      "metadata": {
        "id": "WdR-PSIim1_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of calculate similarity\n",
        "print(word_vectors.similarity('women', 'men'))\n",
        "print(word_vectors.similarity('king', 'queen'))\n",
        "print(word_vectors.similarity('uncle', 'aunt'))\n",
        "print(word_vectors.similarity('boy', 'girl'))\n",
        "print(word_vectors.similarity('paper', 'water'))"
      ],
      "metadata": {
        "id": "j3PhiOKfmzRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finding the similar words\n",
        "print(word_vectors.most_similar('tower', topn=10))\n",
        "\n",
        "print(word_vectors.most_similar('rock', topn=10))"
      ],
      "metadata": {
        "id": "xRb4hREqnZbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jshs09RimeMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZwtXgUGbmeI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now done with the demo\n",
        "now Ceating Token Embeddings"
      ],
      "metadata": {
        "id": "dczZrWOiCeEo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's illustrate how the token ID to embedding vector conversion works with hands on example. Suppose we have the following four input tokens with IDs 2, 3, 5, 1"
      ],
      "metadata": {
        "id": "DrKfTC4hCjpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "input_ids = torch.tensor([2, 3, 5, 1])"
      ],
      "metadata": {
        "id": "8wEaPEJrCff7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the sake of simplicity and illustration purpose, suppose we have a small vocabulary of only 6 words(instead of the 50,257 words in the BPE tokenizer vocabulary), and we want to create embeddings of size 3 dim (in GPT-3, the embedding size is 12,288 embeddings)"
      ],
      "metadata": {
        "id": "aVkTLkpdDIga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the vocab_size and and output_dim, we can instantiate an embedding layer in PyTOrch. setting random seed=123 for reproducibility purpose."
      ],
      "metadata": {
        "id": "_Gb9m4igEIdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 6\n",
        "output_dim = 3\n",
        "\n",
        "torch.manual_seed(123)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ],
      "metadata": {
        "id": "nl-kLZ7KCfb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The print statement in the code prrints the embedding layer's underlying weight matrix:"
      ],
      "metadata": {
        "id": "OUxH2IkRGJmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer)\n",
        "print(embedding_layer.weight)"
      ],
      "metadata": {
        "id": "Z6I5ldG0CfX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the weight matrix of the embedding layer contains small \"random values\". These values are optimized during LLM training as part of the LLM optimization itself, as we will see in the upcoming chapters.Moreover, we can see that the weight matrix has six rows and three columns. There is one row for each of the six possible tokens in the vocabulary. And there is one column for each of the three embedding dimensions."
      ],
      "metadata": {
        "id": "rtmX5eD6GUHg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After we instatiated the embedding layer, let's now apply it to any single token to obtain the embedding vector for that token ID let's see:"
      ],
      "metadata": {
        "id": "RCmJ7NObHpoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(torch.tensor(3)))"
      ],
      "metadata": {
        "id": "EjbGe4SKCew2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we compare the embedding vector for the token ID 3 to the previous embedding matrix, we see that it is identical to the 4th row(Python start with zero indexing, it is the row correspoding to the index3).\n",
        "\n",
        "In other words, the embedding layer is essentially a look-up operation that retrives rows from the embedding layer's weight matrix via token ID.\n"
      ],
      "metadata": {
        "id": "N7tk6bMJIoFu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above we have seen how to convert single token ID into a three-dim embd vector. Let's now apply it to a input_ids (torch.tensor([2, 3, 5, 1])).\n"
      ],
      "metadata": {
        "id": "FcjxJlT1MUFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(input_ids))"
      ],
      "metadata": {
        "id": "9bs4KQLvoaMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each row in this output matrix is obtained via a lookup operation from the embedding weight matrix."
      ],
      "metadata": {
        "id": "gbqlzm-GHth0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oTuLLyfRmT8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wlYc2VdYmT4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NXXVRbKCmT2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lecture"
      ],
      "metadata": {
        "id": "CogsUbbdmqB3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##POSITIONAL EMBEDDINGS(ENCODING WORD POSITIONS)"
      ],
      "metadata": {
        "id": "VnRsdbiOH6zj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Previously, we focused on very small embedding sizes in this chapter for illustration purpose.\n",
        "\n",
        "We now consider more realistic and useful ebedding sizes and encode the input tokens into a 256-dimensional vector representation.\n",
        "\n",
        "This is smaller than what the original GPT-3 model used (in GPT-3, the embedding size is 12,288 dimensions) but still reasonable for experimentation.\n",
        "\n",
        "Furthermore, we assume that the token IDs were created by the BPE tokenizer that we implmeneted ealier, which has a vocabulary size of 50,257:"
      ],
      "metadata": {
        "id": "t7V9zEnFIHyc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ask to GPT:\n",
        "\n",
        "What is the vector embedding size for GPT2 or GPT3 ?\n",
        "\n",
        "What is the vocabulary size for GPT2 pretraining?"
      ],
      "metadata": {
        "id": "0ibJwmy9Lo3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "DboIXEZSMRhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "\n",
        "#It takes two input.....torch.nn.Embedding(num_embeddings, embedding_dim)\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "\n",
        "print(token_embedding_layer.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnAaxez7MTpF",
        "outputId": "a0e0897a-226e-4efe-f20f-4bd851d3534d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 1.0194e+00,  1.2845e+00,  2.3843e-01,  ..., -2.5092e-01,\n",
            "         -3.2715e-01,  5.1654e-04],\n",
            "        [-1.8763e-01,  1.1083e+00, -1.4993e+00,  ..., -3.1205e-01,\n",
            "          1.0904e+00, -1.2905e+00],\n",
            "        [ 5.4586e-01,  3.6950e-01, -8.0691e-01,  ...,  4.5755e-01,\n",
            "         -2.7557e-01,  4.2744e-01],\n",
            "        ...,\n",
            "        [ 2.9920e-01,  8.6643e-01, -5.8656e-01,  ...,  4.5017e-01,\n",
            "         -5.2731e-02,  1.3625e+00],\n",
            "        [ 7.5357e-01,  1.6395e-01,  1.0407e+00,  ..., -2.0124e+00,\n",
            "          1.8168e+00, -2.2102e-01],\n",
            "        [-2.1088e+00, -5.2424e-01,  1.5862e+00,  ...,  6.3221e-01,\n",
            "          2.0748e+00,  8.5035e-01]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the token_embedding_layer above, if we sample data from the data loader, we embed each token in each batch into a 256-dim vector.\n",
        "\n",
        "If we have a batch size of 8 and 4 tokens each, the result will be an 8x4x256."
      ],
      "metadata": {
        "id": "JRk7r9VFNVoN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BdLNVHc6RDSD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nz0I66SARDNn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's instantiate the dataloader (Data sampling with a sliding window) first:"
      ],
      "metadata": {
        "id": "TfuzA0IkNUCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader(raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "\n",
        "inputs, targets = next(data_iter)"
      ],
      "metadata": {
        "id": "h6tkbC5xMQsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Inputs shape:\\n\", inputs.shape)   # 2D tensor of shape 8*4\n",
        "print(\"Token IDs: \\n\", inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMZXIu23o1z0",
        "outputId": "d57c54e4-85f2-4b2e-a004-e112934386a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs shape:\n",
            " torch.Size([8, 4])\n",
            "Token IDs: \n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see that, the token ID tensor is 8*4-dimensional, meaning that the data batch consists of 8 text sample with 4 tokens each."
      ],
      "metadata": {
        "id": "eE9WwZBGphRb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now use the embedding layer to embed these tokens IDs into 256-dimensional vector:"
      ],
      "metadata": {
        "id": "OpMKpNUZp3BW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "token embeddings"
      ],
      "metadata": {
        "id": "Tb6XmGQwz1-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_embeddings = token_embedding_layer(inputs)\n",
        "# print(token_embeddings)\n",
        "print(token_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BTbY2J8pOGi",
        "outputId": "730f14ee-c52b-4b18-e9a5-cc420850d2e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can tell based on the 8x4x256-dimensional tensor output, each token ID is now embedded as a 256-dimensional vector."
      ],
      "metadata": {
        "id": "f7g6M-Ou0Imd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fot GPY model we use absolute encoding approach"
      ],
      "metadata": {
        "id": "zFJu7W9y55WS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)  #4, 256"
      ],
      "metadata": {
        "id": "Oi08Byk-0AG3"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_embedding = pos_embedding_layer(torch.arange(max_length))\n",
        "print(pos_embedding.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvANQMrn6RMk",
        "outputId": "0743756e-8d18-449f-d49f-1aed47aaccc7"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As shown in the preceding code example, the input to the pos_embeddings is usually a placeholder vector torch.arange(context_length), it will create the vector sequences of numbers 0, 1,... utpo maximum input length -1 (means max_length - 1) so here 0,1,2,3..total 4 vectors"
      ],
      "metadata": {
        "id": "6W5OOurvGRst"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the positional embedding tensor consists of four 256-dimensional vectors. We can now add these directly to the token embedding, where PyTorch will add the 4x256-dim pos_embeddings tensor to each 4x256-dim token embeddings tensor in each of the 8 batches:"
      ],
      "metadata": {
        "id": "UKovBvEnHEp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_embeddings = token_embeddings + pos_embedding\n",
        "print(input_embeddings.shape)"
      ],
      "metadata": {
        "id": "2tFcPIU96gc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7988f0ab-3958-4a63-e6de-554a0b58ecf3"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The input_embeddings we have created are the embedded input examples that now can be processed by the model"
      ],
      "metadata": {
        "id": "Peyf_yh_H-A7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qhby5xHKH7lf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}